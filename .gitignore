#importing libraries.
from selenium import webdriver 
from webdriver_manager.chrome import ChromeDriverManager
#collecting rows and headers after setting up our tools.
#headers
for i in range(1,6):
    titles = driver.find_element_by_xpath('//*[@id="notice"]/thead/tr/th['+str(i)+']').text.strip()
    if titles == "\xa0":
        continue
    headers.append(titles)
#for some details we need to create columns manually.
headers.append('Procedure', 'Type', 'Buyer')
#looping the script so that all of the pages can be scraped with our required data.
for z in range(22000):
    for y in range (1,26):
        document_number = driver.find_element_by_xpath('//*[@id="notice"]/tbody/tr['+str(y)+']/td[2]/a').text
        description = driver.find_element_by_xpath('//*[@id="notice"]/tbody/tr['+str(y)+']/td[3]/span[1]').text
        buyer = driver.find_element_by_xpath('//*[@id="notice"]/tbody/tr['+str(y)+']/td[3]/span[2]').text
        procedure = driver.find_element_by_xpath('//*[@id="notice"]/tbody/tr['+str(y)+']/td[3]/span[4]').text
        type = driver.find_element_by_xpath('//*[@id="notice"]/tbody/tr['+str(y)+']/td[3]/span[5]').text
        country = driver.find_element_by_xpath('//*[@id="notice"]/tbody/tr['+str(y)+']/td[4]').text
        publication_date = driver.find_element_by_xpath('//*[@id="notice"]/tbody/tr['+str(y)+']/td[5]').text
        table_data.append([document_number, description, country, publication_date, procedure, type, buyer])
    page_button = driver.find_element_by_xpath('//*[@id="middle-column"]/div/div/div[3]/div/div[4]/div/div[2]/div[11]/a')
    page_button.click()
#creating our data frame for each loop session.
df = pd.DataFrame(data= table_data, columns= headers)
